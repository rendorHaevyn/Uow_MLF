== INDEX ==
1. Week 1 - iPython and GraphLab
2. Week 2 - Linear Regression Modeling
3. Week 3 - Classification Modeling
4. Week 4 - Clustering Analysis
5. Week 5 - Recommender Systems


== Week 1 - iPython and GraphLab ==

import graphlab as gl
sf = SFrame('<<inputfile.csv>>')
sf.head(5)
sf.show() # create web dashboad for data / models etc
gl.canvas.set_target('ipynb') # all visualisations will target this notebook
sf['age'].show(view='Categorical') # print graph of age column


== Week 2 - Linear Regression Modeling ==
> f(w)x = w0 + w1x
* function parameterised by features w
* we assess the cost of a given model by the Residual Sum of Squares (RSS), predicted WRT actual
* we minimise this cost over all possible w0 and w1
> adding higher order (quadratic) features still qualifies prediction models as linear regression
> Overfitting
* we need to avoid overfitting
* can do so by creating a training and test data set,and plotting model error on training and test dataset, so model complexity X model error
* NOTE: should consider validation dataset also
> Machine learning block model:
* Data --> Feature Extraction (x) --> ML Model (w^ or θ's) --> Prediction of Response (y^) --> Compare with actual response with a quality metric (goodness of fit metric), such as Error RSS --> update ML algorithm & w^ --> Prediction of Response... << continues >>
* w^ (θ's) are weights on the features, AKA "regression coefficients"


== Week 3 - Classification Modeling ==
> Linear classifiers of sentiment analysis:
* We can use a linear classifier to learn weights for each word
* Words assigned "good" or "bad" from a sentence can thus be weighted in order to evaluate a user's feedback
* We call this a linear classifier because the output is the weighted sum of the inputs
* Linear decision boundary separates positive and negative predictions
* For linear classifiers:
  ** When 2 weights are non-zero, the decision boundary is a line
  ** When 3 weights are non-zero, the decision boundary is a plane
  ** When 4+ weights are non-zero, the decision boundary is a hyperplane
* For more general (non-linear) classifiers, the decision boundary can take on more complex shapes

> Measuring model prediction with accuracy:
* If there is class imbalance, accuracy alone is not enough
* eg: If model predicts 90% of emails are spam, yet we know that 90% of emails *are* spam, then while the model "looks" good from an accuracy perspective, it's not providing more insight than simply guessing that every email is spam!

> Confusion matrices:
* Allow us to plot out false negative and false positive rates
* Typically work with two classes (binary classification) but can also work with multiclass classification

> Learning Curves:
* Despite infinite data, the error in predicting a test dataset output will still have error
* This error is known as bias
* More complex models are likely to have less bias, however, the require more data to learn

> Machine learning block model for Classification:
*   Data                        = Text of the restaurant review / sentences-->
    Feature Extraction (x)      = Word Counts -->
    ML Model (w^ or θ's)        = Word Weights -->
    Prediction of Response (y^) = Predicted Sentiment -->
    Compare with actual response with a quality metric (goodness of fit metric), such as Error RSS -->
    Update ML algorithm & w^ --> Prediction of Response
    ... << continues >>


== Week 4 - Clustering Analysis ==
> Word count representation:
> Bag of words model
* ignores word order
* count # instances of each word
> Finding related articles
* We can build a word instance vector and multiply this across articles to get a similarity score
* Problem: the score varies with the length of the articles
* Solution: normalise these input vectors

> We can estimate the importance of words by using a "Term Frequency : Inverse Document Frequency" approach, TF-IDF
* TF-IDF Word Weight = log( [# Total Documents] / [1 + # Documents Using the Word] )
** ie: the more documents this word appears in, the closer its weight moves towards 0

> K-nearest neighbour will return similar documents based upon a score related to word features
> Clustering
* Unsupervised learning, where we want to discover cluster structure
* Voronoi Tesselation is a technique to assist with explaining nearest neighbour clusters

> Machine learning block model for Clustering and Similarity:
*   Data                        = Document ID / Text table -->
    Feature Extraction (x)      = TF-IDF vector -->
    ML Model (w^ or θ's)        = Clustering -->
    Prediction of Response (y^) = Predicted Cluster Label -->
    Minimise error of distance between cluster centroids and observed values  -->
    Update ML algorithm & w^ --> Prediction of Response
    ... << continues >>


== Week 5 - Recommender Systems ==
> Recommender systems have diverse applications, such as re-purposing/targeting drugs to other applications / targets / sites
> Collaborative filtering:
* build matrices which overlay product preferences based on other products
* popular items will drown out other items in recommender systems if not adjusted for, so we adjust for popularity by:
1. Jaccard Similarity - normalise by dividing,
        count of co-purchase of Product A **and** Product B / count of purchase of Product A **or** Product B
    > but, this has limitations
2. Weighted average - but, this has limitations, ie: cold-start problem - we have no usage history for new products / users
* we can make movie recommendations simply by element-wise multiplication of user genre preference vectors by movie feature vectors
> Machine learning block model for Recommender using Matrix Factorisation:
*   Data                        = User / Produt / Ratings table -->
    Feature Extraction (x)      = User ID / Product ID / Age / Gender / Description... -->
    ML Model (w^ or θ's)        = Matrix Factorisation -->
    Prediction of Response (y^) = Predicted Rating -->
    Minimise error between actual rating and predictive ratings -->
    w^ = estimated parameters, inc weights, feature vectors for each user and product  -->
    ... << continues >>








